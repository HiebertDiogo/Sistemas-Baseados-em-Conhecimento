{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e03bbc",
   "metadata": {},
   "source": [
    "#  Processamento de Linguagem natural (PLN) - PARTE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "213e4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "##### Notebook Processamento de Linguagem natural (PLN)\n",
    "##### Baseado em:\n",
    "## Natural Language Processing with Python (book)\n",
    "##\n",
    "##############################################################################################################\n",
    "## Objetivos:\n",
    "##   Mostrar aplicações de metodos de linguagem natural aprendidos em aula\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b9b9c",
   "metadata": {},
   "source": [
    "## Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80662ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2416b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Mostra o conteúdo completo das colunas\n",
    "pd.set_option('display.max_columns', 100)  # Mostra o conteúdo completo das colunas\n",
    "pd.set_option('display.max_rows', 100)  # Mostra o conteúdo completo das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0a4da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'tweet'],\n",
      "    num_rows: 31962\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"tweets_hate_speech_detection\",  split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "# Conversão para DataFrame do Pandas\n",
    "dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8805ff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>loving this conference call that i am on with one of my teammates.big things are happening within our team!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>happy father's day to all dads! #fathersday   #family #cooldad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>great to be able to have a catchup with my lovely agent @user today and discuss happenings for the future!    actor :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>this song brings back so many memories ð #memories #thegoodolddays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>always be   #whitesides</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     tweet\n",
       "95          loving this conference call that i am on with one of my teammates.big things are happening within our team!   \n",
       "96                                                         happy father's day to all dads! #fathersday   #family #cooldad \n",
       "97  great to be able to have a catchup with my lovely agent @user today and discuss happenings for the future!    actor :)\n",
       "98                                                this song brings back so many memories ð #memories #thegoodolddays   \n",
       "99                                                                                                always be   #whitesides "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selectionando uma amostra\n",
    "df = dataset.sample(n=100, random_state=21)[[\"tweet\"]].reset_index(drop=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc593b",
   "metadata": {},
   "source": [
    "## Tokeninzação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7670e",
   "metadata": {},
   "source": [
    "> Método 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0f05defb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>WhitespaceTokenizer</th>\n",
       "      <th>WordPunctTokenizer</th>\n",
       "      <th>TreebankWordTokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when cinema is an expression of freedom or cinema reflecting societal issues but india is not ready to show itself a mirror   #udtapunjab</td>\n",
       "      <td>[when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #udtapunjab]</td>\n",
       "      <td>[when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]</td>\n",
       "      <td>[when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how far is #europe swinging to the right? - #nyt  #libcrib #uniteblue #fascism #trump #fear #ignorance  #hate</td>\n",
       "      <td>[how, far, is, #europe, swinging, to, the, right?, -, #nyt, #libcrib, #uniteblue, #fascism, #trump, #fear, #ignorance, #hate]</td>\n",
       "      <td>[how, far, is, #, europe, swinging, to, the, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]</td>\n",
       "      <td>[how, far, is, #, europe, swinging, to, the, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for a lot of people it's their paner who has to compete against the twitter.   but #true</td>\n",
       "      <td>[for, a, lot, of, people, it's, their, paner, who, has, to, compete, against, the, twitter., but, #true]</td>\n",
       "      <td>[for, a, lot, of, people, it, ', s, their, paner, who, has, to, compete, against, the, twitter, ., but, #, true]</td>\n",
       "      <td>[for, a, lot, of, people, it, 's, their, paner, who, has, to, compete, against, the, twitter., but, #, true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rest in peace christina grimmie. i loved your voice and your youtube covers â¤ï¸ #restinlovechristina    #shocked #restinpiecechristina</td>\n",
       "      <td>[rest, in, peace, christina, grimmie., i, loved, your, voice, and, your, youtube, covers, â¤ï¸, #restinlovechristina, #shocked, #restinpiecechristina]</td>\n",
       "      <td>[rest, in, peace, christina, grimmie, ., i, loved, your, voice, and, your, youtube, covers, â, ¤, ï, ¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]</td>\n",
       "      <td>[rest, in, peace, christina, grimmie., i, loved, your, voice, and, your, youtube, covers, â¤ï¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>come on england!!! â½ï¸â½ï¸â½ï¸ #euro2016 #england #football</td>\n",
       "      <td>[come, on, england!!!, â½ï¸â½ï¸â½ï¸, #euro2016, #england, #football]</td>\n",
       "      <td>[come, on, england, !!!, â, , ½ï, ¸, â, , ½ï, ¸, â, , ½ï, ¸, #, euro2016, #, england, #, football]</td>\n",
       "      <td>[come, on, england, !, !, !, â½ï¸â½ï¸â½ï¸, #, euro2016, #, england, #, football]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       tweet  \\\n",
       "0  when cinema is an expression of freedom or cinema reflecting societal issues but india is not ready to show itself a mirror   #udtapunjab   \n",
       "1                              how far is #europe swinging to the right? - #nyt  #libcrib #uniteblue #fascism #trump #fear #ignorance  #hate   \n",
       "2                                                  for a lot of people it's their paner who has to compete against the twitter.   but #true    \n",
       "3  rest in peace christina grimmie. i loved your voice and your youtube covers â¤ï¸ #restinlovechristina    #shocked #restinpiecechristina   \n",
       "4                                                                      come on england!!! â½ï¸â½ï¸â½ï¸ #euro2016 #england #football      \n",
       "\n",
       "                                                                                                                                               WhitespaceTokenizer  \\\n",
       "0  [when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #udtapunjab]   \n",
       "1                                    [how, far, is, #europe, swinging, to, the, right?, -, #nyt, #libcrib, #uniteblue, #fascism, #trump, #fear, #ignorance, #hate]   \n",
       "2                                                         [for, a, lot, of, people, it's, their, paner, who, has, to, compete, against, the, twitter., but, #true]   \n",
       "3         [rest, in, peace, christina, grimmie., i, loved, your, voice, and, your, youtube, covers, â¤ï¸, #restinlovechristina, #shocked, #restinpiecechristina]   \n",
       "4                                                                                       [come, on, england!!!, â½ï¸â½ï¸â½ï¸, #euro2016, #england, #football]   \n",
       "\n",
       "                                                                                                                                                       WordPunctTokenizer  \\\n",
       "0       [when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]   \n",
       "1                       [how, far, is, #, europe, swinging, to, the, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]   \n",
       "2                                                        [for, a, lot, of, people, it, ', s, their, paner, who, has, to, compete, against, the, twitter, ., but, #, true]   \n",
       "3  [rest, in, peace, christina, grimmie, ., i, loved, your, voice, and, your, youtube, covers, â, ¤, ï, ¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]   \n",
       "4                                                                [come, on, england, !!!, â, , ½ï, ¸, â, , ½ï, ¸, â, , ½ï, ¸, #, euro2016, #, england, #, football]   \n",
       "\n",
       "                                                                                                                                               TreebankWordTokenizer  \n",
       "0  [when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]  \n",
       "1                  [how, far, is, #, europe, swinging, to, the, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]  \n",
       "2                                                       [for, a, lot, of, people, it, 's, their, paner, who, has, to, compete, against, the, twitter., but, #, true]  \n",
       "3     [rest, in, peace, christina, grimmie., i, loved, your, voice, and, your, youtube, covers, â¤ï¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]  \n",
       "4                                                                             [come, on, england, !, !, !, â½ï¸â½ï¸â½ï¸, #, euro2016, #, england, #, football]  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "\n",
    "# Convertendo para minúsculas, para \"normalização\"\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "\n",
    "tokenizers = {\n",
    "    \"WhitespaceTokenizer\": WhitespaceTokenizer(),\n",
    "    \"WordPunctTokenizer\": WordPunctTokenizer(),\n",
    "    \"TreebankWordTokenizer\": TreebankWordTokenizer()\n",
    "}\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    df[name] = df[\"tweet\"].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64505d",
   "metadata": {},
   "source": [
    "> Método 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1ec3e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>WhitespaceTokenizer</th>\n",
       "      <th>WordPunctTokenizer</th>\n",
       "      <th>TreebankWordTokenizer</th>\n",
       "      <th>nltk_word_tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>loving this conference call that i am on with one of my teammates.big things are happening within our team!</td>\n",
       "      <td>[loving, this, conference, call, that, i, am, on, with, one, of, my, teammates.big, things, are, happening, within, our, team!]</td>\n",
       "      <td>[loving, this, conference, call, that, i, am, on, with, one, of, my, teammates, ., big, things, are, happening, within, our, team, !]</td>\n",
       "      <td>[loving, this, conference, call, that, i, am, on, with, one, of, my, teammates.big, things, are, happening, within, our, team, !]</td>\n",
       "      <td>[loving, this, conference, call, that, i, am, on, with, one, of, my, teammates.big, things, are, happening, within, our, team, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>happy father's day to all dads! #fathersday   #family #cooldad</td>\n",
       "      <td>[happy, father's, day, to, all, dads!, #fathersday, #family, #cooldad]</td>\n",
       "      <td>[happy, father, ', s, day, to, all, dads, !, #, fathersday, #, family, #, cooldad]</td>\n",
       "      <td>[happy, father, 's, day, to, all, dads, !, #, fathersday, #, family, #, cooldad]</td>\n",
       "      <td>[happy, father, 's, day, to, all, dads, !, #, fathersday, #, family, #, cooldad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>great to be able to have a catchup with my lovely agent @user today and discuss happenings for the future!    actor :)</td>\n",
       "      <td>[great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @user, today, and, discuss, happenings, for, the, future!, actor, :)]</td>\n",
       "      <td>[great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @, user, today, and, discuss, happenings, for, the, future, !, actor, :)]</td>\n",
       "      <td>[great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @, user, today, and, discuss, happenings, for, the, future, !, actor, :, )]</td>\n",
       "      <td>[great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @, user, today, and, discuss, happenings, for, the, future, !, actor, :, )]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>this song brings back so many memories ð #memories #thegoodolddays</td>\n",
       "      <td>[this, song, brings, back, so, many, memories, ð, #memories, #thegoodolddays]</td>\n",
       "      <td>[this, song, brings, back, so, many, memories, ð, , #, memories, #, thegoodolddays]</td>\n",
       "      <td>[this, song, brings, back, so, many, memories, ð, #, memories, #, thegoodolddays]</td>\n",
       "      <td>[this, song, brings, back, so, many, memories, ð, #, memories, #, thegoodolddays]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>always be   #whitesides</td>\n",
       "      <td>[always, be, #whitesides]</td>\n",
       "      <td>[always, be, #, whitesides]</td>\n",
       "      <td>[always, be, #, whitesides]</td>\n",
       "      <td>[always, be, #, whitesides]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     tweet  \\\n",
       "95          loving this conference call that i am on with one of my teammates.big things are happening within our team!      \n",
       "96                                                         happy father's day to all dads! #fathersday   #family #cooldad    \n",
       "97  great to be able to have a catchup with my lovely agent @user today and discuss happenings for the future!    actor :)   \n",
       "98                                                this song brings back so many memories ð #memories #thegoodolddays      \n",
       "99                                                                                                always be   #whitesides    \n",
       "\n",
       "                                                                                                                           WhitespaceTokenizer  \\\n",
       "95             [loving, this, conference, call, that, i, am, on, with, one, of, my, teammates.big, things, are, happening, within, our, team!]   \n",
       "96                                                                      [happy, father's, day, to, all, dads!, #fathersday, #family, #cooldad]   \n",
       "97  [great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @user, today, and, discuss, happenings, for, the, future!, actor, :)]   \n",
       "98                                                            [this, song, brings, back, so, many, memories, ð, #memories, #thegoodolddays]   \n",
       "99                                                                                                                   [always, be, #whitesides]   \n",
       "\n",
       "                                                                                                                                WordPunctTokenizer  \\\n",
       "95           [loving, this, conference, call, that, i, am, on, with, one, of, my, teammates, ., big, things, are, happening, within, our, team, !]   \n",
       "96                                                              [happy, father, ', s, day, to, all, dads, !, #, fathersday, #, family, #, cooldad]   \n",
       "97  [great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @, user, today, and, discuss, happenings, for, the, future, !, actor, :)]   \n",
       "98                                                          [this, song, brings, back, so, many, memories, ð, , #, memories, #, thegoodolddays]   \n",
       "99                                                                                                                     [always, be, #, whitesides]   \n",
       "\n",
       "                                                                                                                               TreebankWordTokenizer  \\\n",
       "95                 [loving, this, conference, call, that, i, am, on, with, one, of, my, teammates.big, things, are, happening, within, our, team, !]   \n",
       "96                                                                  [happy, father, 's, day, to, all, dads, !, #, fathersday, #, family, #, cooldad]   \n",
       "97  [great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @, user, today, and, discuss, happenings, for, the, future, !, actor, :, )]   \n",
       "98                                                              [this, song, brings, back, so, many, memories, ð, #, memories, #, thegoodolddays]   \n",
       "99                                                                                                                       [always, be, #, whitesides]   \n",
       "\n",
       "                                                                                                                                  nltk_word_tokenize  \n",
       "95                 [loving, this, conference, call, that, i, am, on, with, one, of, my, teammates.big, things, are, happening, within, our, team, !]  \n",
       "96                                                                  [happy, father, 's, day, to, all, dads, !, #, fathersday, #, family, #, cooldad]  \n",
       "97  [great, to, be, able, to, have, a, catchup, with, my, lovely, agent, @, user, today, and, discuss, happenings, for, the, future, !, actor, :, )]  \n",
       "98                                                              [this, song, brings, back, so, many, memories, ð, #, memories, #, thegoodolddays]  \n",
       "99                                                                                                                       [always, be, #, whitesides]  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "# Percebe-se que o método word_tokenize é baseado no TreebankWordTokenizer, mas com algumas melhorias.\n",
    "df['nltk_word_tokenize'] = df['tweet'].apply(tokenize.word_tokenize)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf89b7e",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c39e76",
   "metadata": {},
   "source": [
    "Stemming reduz a palavra, removendo sufixos e prefixos, podendo não gerar uma palavra válida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e00b5e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk_word_tokenize    [when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]\n",
      "stemmed                         [when, cinema, is, an, express, of, freedom, or, cinema, reflect, societ, issu, but, india, is, not, readi, to, show, itself, a, mirror, #, udtapunjab]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['nltk_word_tokenize'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(df.loc[0 ,['nltk_word_tokenize', 'stemmed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583902c0",
   "metadata": {},
   "source": [
    "## Lematization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d9cf9",
   "metadata": {},
   "source": [
    "Lemmatization devolve a palavra raiz, considerando o contexto e o significado da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "820dc9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Masmok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk_word_tokenize    [@, user, #, gogirl, #, summer, #, camps, #, cardiff, #, girls, 10-17, #, confident, book, now, pls]\n",
      "lemmatized              [@, user, #, gogirl, #, summer, #, camp, #, cardiff, #, girl, 10-17, #, confident, book, now, pls]\n",
      "Name: 71, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') # dicionário de sinônimos\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['nltk_word_tokenize'].apply(lambda x: [wnl.lemmatize(word) for word in x])\n",
    "print(df.loc[71 ,['nltk_word_tokenize', 'lemmatized']])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64b09f",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195867e8",
   "metadata": {},
   "source": [
    "Stopwords são palavras comuns que geralmente não agregam significado relevante ao texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "396c97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Masmok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nltk_word_tokenize</th>\n",
       "      <th>without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]</td>\n",
       "      <td>[cinema, expression, freedom, cinema, reflecting, societal, issues, india, ready, show, mirror, #, udtapunjab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[how, far, is, #, europe, swinging, to, the, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]</td>\n",
       "      <td>[far, #, europe, swinging, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[for, a, lot, of, people, it, 's, their, paner, who, has, to, compete, against, the, twitter, ., but, #, true]</td>\n",
       "      <td>[lot, people, 's, paner, compete, twitter, ., #, true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rest, in, peace, christina, grimmie, ., i, loved, your, voice, and, your, youtube, covers, â¤ï¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]</td>\n",
       "      <td>[rest, peace, christina, grimmie, ., loved, voice, youtube, covers, â¤ï¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[come, on, england, !, !, !, â½ï¸â½ï¸â½ï¸, #, euro2016, #, england, #, football]</td>\n",
       "      <td>[come, england, !, !, !, â½ï¸â½ï¸â½ï¸, #, euro2016, #, england, #, football]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  nltk_word_tokenize  \\\n",
       "0  [when, cinema, is, an, expression, of, freedom, or, cinema, reflecting, societal, issues, but, india, is, not, ready, to, show, itself, a, mirror, #, udtapunjab]   \n",
       "1                  [how, far, is, #, europe, swinging, to, the, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]   \n",
       "2                                                     [for, a, lot, of, people, it, 's, their, paner, who, has, to, compete, against, the, twitter, ., but, #, true]   \n",
       "3   [rest, in, peace, christina, grimmie, ., i, loved, your, voice, and, your, youtube, covers, â¤ï¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]   \n",
       "4                                                                             [come, on, england, !, !, !, â½ï¸â½ï¸â½ï¸, #, euro2016, #, england, #, football]   \n",
       "\n",
       "                                                                                                                          without_stopwords  \n",
       "0                            [cinema, expression, freedom, cinema, reflecting, societal, issues, india, ready, show, mirror, #, udtapunjab]  \n",
       "1           [far, #, europe, swinging, right, ?, -, #, nyt, #, libcrib, #, uniteblue, #, fascism, #, trump, #, fear, #, ignorance, #, hate]  \n",
       "2                                                                                    [lot, people, 's, paner, compete, twitter, ., #, true]  \n",
       "3  [rest, peace, christina, grimmie, ., loved, voice, youtube, covers, â¤ï¸, #, restinlovechristina, #, shocked, #, restinpiecechristina]  \n",
       "4                                                        [come, england, !, !, !, â½ï¸â½ï¸â½ï¸, #, euro2016, #, england, #, football]  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['without_stopwords'] = df['nltk_word_tokenize'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "df[['nltk_word_tokenize', 'without_stopwords']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf5828",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e654c7",
   "metadata": {},
   "source": [
    "### Tfidf - Caracterizacao das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4619f6",
   "metadata": {},
   "source": [
    " Tfidf vai avaliar a importância de uma palavra em um tweet, considerando a frequência da palavra no próprio tweet e em todo o corpus\n",
    "\n",
    " Quanto mais rara a palavra no corpus, maior será seu peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e8cfed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>affirmation</th>\n",
       "      <th>all</th>\n",
       "      <th>always</th>\n",
       "      <th>am</th>\n",
       "      <th>am thankful</th>\n",
       "      <th>amp</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>are you</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>back</th>\n",
       "      <th>be</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>because</th>\n",
       "      <th>big</th>\n",
       "      <th>bihday</th>\n",
       "      <th>bihday to</th>\n",
       "      <th>business</th>\n",
       "      <th>but</th>\n",
       "      <th>can</th>\n",
       "      <th>can find</th>\n",
       "      <th>christina</th>\n",
       "      <th>christina grimmie</th>\n",
       "      <th>color</th>\n",
       "      <th>confident</th>\n",
       "      <th>cute</th>\n",
       "      <th>day</th>\n",
       "      <th>do</th>\n",
       "      <th>ever</th>\n",
       "      <th>family</th>\n",
       "      <th>feel</th>\n",
       "      <th>feeling</th>\n",
       "      <th>find</th>\n",
       "      <th>first</th>\n",
       "      <th>flower</th>\n",
       "      <th>for</th>\n",
       "      <th>for all</th>\n",
       "      <th>for the</th>\n",
       "      <th>from</th>\n",
       "      <th>fun</th>\n",
       "      <th>game</th>\n",
       "      <th>gay</th>\n",
       "      <th>getting</th>\n",
       "      <th>girls</th>\n",
       "      <th>glad</th>\n",
       "      <th>going</th>\n",
       "      <th>...</th>\n",
       "      <th>sun</th>\n",
       "      <th>sunday</th>\n",
       "      <th>team</th>\n",
       "      <th>thankful</th>\n",
       "      <th>thankful for</th>\n",
       "      <th>thankful positive</th>\n",
       "      <th>thanks</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>the sun</th>\n",
       "      <th>their</th>\n",
       "      <th>them</th>\n",
       "      <th>thinking</th>\n",
       "      <th>this</th>\n",
       "      <th>time</th>\n",
       "      <th>to</th>\n",
       "      <th>to be</th>\n",
       "      <th>to see</th>\n",
       "      <th>to the</th>\n",
       "      <th>to you</th>\n",
       "      <th>today</th>\n",
       "      <th>truth</th>\n",
       "      <th>udtapunjab</th>\n",
       "      <th>up</th>\n",
       "      <th>up with</th>\n",
       "      <th>user</th>\n",
       "      <th>user oh</th>\n",
       "      <th>user today</th>\n",
       "      <th>user user</th>\n",
       "      <th>waiting</th>\n",
       "      <th>warm</th>\n",
       "      <th>was</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>weeks</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>who</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>with my</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361062</td>\n",
       "      <td>0.361062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  affirmation  all  always   am  am thankful  amp        an       and  \\\n",
       "0    0.0          0.0  0.0     0.0  0.0          0.0  0.0  0.299156  0.000000   \n",
       "1    0.0          0.0  0.0     0.0  0.0          0.0  0.0  0.000000  0.000000   \n",
       "2    0.0          0.0  0.0     0.0  0.0          0.0  0.0  0.000000  0.000000   \n",
       "3    0.0          0.0  0.0     0.0  0.0          0.0  0.0  0.000000  0.205501   \n",
       "4    0.0          0.0  0.0     0.0  0.0          0.0  0.0  0.000000  0.000000   \n",
       "\n",
       "   any  are  are you   as   at  back   be  beautiful  because  big  bihday  \\\n",
       "0  0.0  0.0      0.0  0.0  0.0   0.0  0.0        0.0      0.0  0.0     0.0   \n",
       "1  0.0  0.0      0.0  0.0  0.0   0.0  0.0        0.0      0.0  0.0     0.0   \n",
       "2  0.0  0.0      0.0  0.0  0.0   0.0  0.0        0.0      0.0  0.0     0.0   \n",
       "3  0.0  0.0      0.0  0.0  0.0   0.0  0.0        0.0      0.0  0.0     0.0   \n",
       "4  0.0  0.0      0.0  0.0  0.0   0.0  0.0        0.0      0.0  0.0     0.0   \n",
       "\n",
       "   bihday to  business       but  can  can find  christina  christina grimmie  \\\n",
       "0        0.0       0.0  0.299156  0.0       0.0   0.000000           0.000000   \n",
       "1        0.0       0.0  0.000000  0.0       0.0   0.000000           0.000000   \n",
       "2        0.0       0.0  0.372165  0.0       0.0   0.000000           0.000000   \n",
       "3        0.0       0.0  0.000000  0.0       0.0   0.361062           0.361062   \n",
       "4        0.0       0.0  0.000000  0.0       0.0   0.000000           0.000000   \n",
       "\n",
       "   color  confident  cute  day   do  ever  family  feel  feeling  find  first  \\\n",
       "0    0.0        0.0   0.0  0.0  0.0   0.0     0.0   0.0      0.0   0.0    0.0   \n",
       "1    0.0        0.0   0.0  0.0  0.0   0.0     0.0   0.0      0.0   0.0    0.0   \n",
       "2    0.0        0.0   0.0  0.0  0.0   0.0     0.0   0.0      0.0   0.0    0.0   \n",
       "3    0.0        0.0   0.0  0.0  0.0   0.0     0.0   0.0      0.0   0.0    0.0   \n",
       "4    0.0        0.0   0.0  0.0  0.0   0.0     0.0   0.0      0.0   0.0    0.0   \n",
       "\n",
       "   flower       for  for all  for the  from  fun  game  gay  getting  girls  \\\n",
       "0     0.0  0.000000      0.0      0.0   0.0  0.0   0.0  0.0      0.0    0.0   \n",
       "1     0.0  0.000000      0.0      0.0   0.0  0.0   0.0  0.0      0.0    0.0   \n",
       "2     0.0  0.244826      0.0      0.0   0.0  0.0   0.0  0.0      0.0    0.0   \n",
       "3     0.0  0.000000      0.0      0.0   0.0  0.0   0.0  0.0      0.0    0.0   \n",
       "4     0.0  0.000000      0.0      0.0   0.0  0.0   0.0  0.0      0.0    0.0   \n",
       "\n",
       "   glad  going  ...  sun  sunday  team  thankful  thankful for  \\\n",
       "0   0.0    0.0  ...  0.0     0.0   0.0       0.0           0.0   \n",
       "1   0.0    0.0  ...  0.0     0.0   0.0       0.0           0.0   \n",
       "2   0.0    0.0  ...  0.0     0.0   0.0       0.0           0.0   \n",
       "3   0.0    0.0  ...  0.0     0.0   0.0       0.0           0.0   \n",
       "4   0.0    0.0  ...  0.0     0.0   0.0       0.0           0.0   \n",
       "\n",
       "   thankful positive  thanks  that      the  the sun     their  them  \\\n",
       "0                0.0     0.0   0.0  0.00000      0.0  0.000000   0.0   \n",
       "1                0.0     0.0   0.0  0.28457      0.0  0.000000   0.0   \n",
       "2                0.0     0.0   0.0  0.22623      0.0  0.372165   0.0   \n",
       "3                0.0     0.0   0.0  0.00000      0.0  0.000000   0.0   \n",
       "4                0.0     0.0   0.0  0.00000      0.0  0.000000   0.0   \n",
       "\n",
       "   thinking  this  time        to  to be  to see    to the  to you  today  \\\n",
       "0       0.0   0.0   0.0  0.178559    0.0     0.0  0.000000     0.0    0.0   \n",
       "1       0.0   0.0   0.0  0.279421    0.0     0.0  0.499987     0.0    0.0   \n",
       "2       0.0   0.0   0.0  0.222136    0.0     0.0  0.000000     0.0    0.0   \n",
       "3       0.0   0.0   0.0  0.000000    0.0     0.0  0.000000     0.0    0.0   \n",
       "4       0.0   0.0   0.0  0.000000    0.0     0.0  0.000000     0.0    0.0   \n",
       "\n",
       "   truth  udtapunjab   up  up with  user  user oh  user today  user user  \\\n",
       "0    0.0    0.319507  0.0      0.0   0.0      0.0         0.0        0.0   \n",
       "1    0.0    0.000000  0.0      0.0   0.0      0.0         0.0        0.0   \n",
       "2    0.0    0.000000  0.0      0.0   0.0      0.0         0.0        0.0   \n",
       "3    0.0    0.000000  0.0      0.0   0.0      0.0         0.0        0.0   \n",
       "4    0.0    0.000000  0.0      0.0   0.0      0.0         0.0        0.0   \n",
       "\n",
       "   waiting  warm  was  way   we  week  weekend  weeks  were  what      when  \\\n",
       "0      0.0   0.0  0.0  0.0  0.0   0.0      0.0    0.0   0.0   0.0  0.283371   \n",
       "1      0.0   0.0  0.0  0.0  0.0   0.0      0.0    0.0   0.0   0.0  0.000000   \n",
       "2      0.0   0.0  0.0  0.0  0.0   0.0      0.0    0.0   0.0   0.0  0.000000   \n",
       "3      0.0   0.0  0.0  0.0  0.0   0.0      0.0    0.0   0.0   0.0  0.000000   \n",
       "4      0.0   0.0  0.0  0.0  0.0   0.0      0.0    0.0   0.0   0.0  0.000000   \n",
       "\n",
       "        who  why  will  with  with my  would  year  yes  you    your  \n",
       "0  0.000000  0.0   0.0   0.0      0.0    0.0   0.0  0.0  0.0  0.0000  \n",
       "1  0.000000  0.0   0.0   0.0      0.0    0.0   0.0  0.0  0.0  0.0000  \n",
       "2  0.352527  0.0   0.0   0.0      0.0    0.0   0.0  0.0  0.0  0.0000  \n",
       "3  0.000000  0.0   0.0   0.0      0.0    0.0   0.0  0.0  0.0  0.6113  \n",
       "4  0.000000  0.0   0.0   0.0      0.0    0.0   0.0  0.0  0.0  0.0000  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1 ,2))\n",
    "tfidf_matrix = tfidf.fit_transform(df['tweet'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853987ec",
   "metadata": {},
   "source": [
    "### Wordvector - Caracterizacao das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8e917687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'love': [-7.63257220e-03 -1.82980392e-03 -7.57275056e-03  8.24001990e-03\n",
      "  6.30763918e-03  2.90567777e-03  8.59658886e-03  2.17109662e-03\n",
      " -9.71628353e-03  8.13894346e-03 -5.55999810e-03  6.30609831e-03\n",
      "  5.76864090e-03 -4.06302221e-04 -7.59733934e-03 -1.65341236e-03\n",
      "  6.61904039e-03 -8.42325576e-03  1.29675248e-03 -1.04370890e-02\n",
      "  9.86397453e-03 -2.24699709e-03  9.89400595e-03 -6.11570990e-03\n",
      " -1.00384932e-02 -8.49537551e-03 -5.04959049e-03  4.24119737e-03\n",
      " -1.02953252e-03  9.18981992e-03  4.34712414e-03  3.76335159e-03\n",
      "  3.81689821e-03  7.15161394e-03 -3.33116343e-03  8.76694545e-03\n",
      " -9.16175265e-03  1.62772636e-03 -1.46711606e-03 -1.20701699e-03\n",
      "  6.59864303e-03 -3.26838368e-03 -2.88574793e-03 -2.19577560e-04\n",
      " -1.37135485e-05 -4.24654363e-03  4.93441476e-03 -6.44675083e-03\n",
      "  8.60239659e-03  7.60134833e-04  2.98946304e-03  2.47849361e-03\n",
      " -4.60073526e-04  1.88129616e-03 -3.65304691e-03  4.99762502e-03\n",
      "  9.57880926e-04 -3.86714796e-03 -9.73483548e-03 -9.09287576e-03\n",
      "  1.95402274e-04 -5.58193121e-03 -1.46483735e-03 -4.36018500e-03\n",
      " -9.63876862e-03  2.15853518e-03  5.82974125e-03 -8.92895972e-04\n",
      " -8.59842729e-03  3.89576796e-03 -7.16515293e-04 -5.07848384e-03\n",
      " -4.57819406e-04 -1.45947270e-03 -1.68336811e-03 -9.44450311e-03\n",
      "  7.81847816e-03  3.52732441e-03 -2.60540005e-03  8.03277735e-03\n",
      " -4.95929271e-04  7.06418697e-03 -2.74401670e-03  9.84384678e-03\n",
      " -7.51366664e-04 -5.99088240e-03 -3.85231501e-03  1.08461566e-02\n",
      "  1.75479637e-03  1.51307683e-03  1.01027675e-02 -5.80288423e-03\n",
      " -1.79007370e-03 -8.00042599e-03 -4.96357027e-03 -7.76997954e-03\n",
      "  4.37259814e-03  1.98859535e-03  6.04774803e-03  2.45249690e-03]\n",
      "Similar words to 'love': [('actor', 0.331228643655777), ('perform', 0.29603174328804016), ('oh', 0.2922592759132385), ('waiting', 0.26749810576438904), ('mirror', 0.26495611667633057), ('football', 0.26171594858169556), ('kp', 0.2577618360519409), (';', 0.2568994462490082), ('asksaad', 0.25365522503852844), ('celebrate', 0.2523535192012787)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# vector_size: Dimensionalidade dos vetores de palavras\n",
    "# window: Quantas palavras antes e depois da palavra alvo serão consideradas\n",
    "# min_count: Ignora palavras com frequência menor que esse valor\n",
    "# workers: Número de threads para treinamento\n",
    "model = Word2Vec(sentences=df['nltk_word_tokenize'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = model.wv\n",
    "\n",
    "print(\"Vector for 'love':\", word_vectors['love'])  # Vetor da palavra 'love'\n",
    "print(\"Similar words to 'love':\", word_vectors.most_similar('love'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
