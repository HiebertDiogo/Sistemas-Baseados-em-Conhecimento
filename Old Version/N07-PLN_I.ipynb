{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e9192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "##### Notebook Processamento de Linguagem natural (PLN)\n",
    "##### Baseado em:\n",
    "## Natural Language Processing with Python (book)\n",
    "##\n",
    "##############################################################################################################\n",
    "## Objetivos:\n",
    "##   Mostrar varios metodos de linguagem natural utilizando Python\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d53311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', \"Mary's\", 'car,', \"isn't\", 'it?']\n",
      "['This', 'is', 'Mary', \"'\", 's', 'car', ',', 'isn', \"'\", 't', 'it', '?']\n",
      "['This', 'is', 'Mary', \"'s\", 'car', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### 01 - Tokenizacao\n",
    "################################################\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"This is Mary's car, isn't it?\"\n",
    "tk_list = []\n",
    "tk_list.append(WhitespaceTokenizer()) \n",
    "tk_list.append(WordPunctTokenizer())\n",
    "tk_list.append(TreebankWordTokenizer())\n",
    "\n",
    "for tk in tk_list:\n",
    "    result = tk.tokenize(text) \n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725d3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guarda-chuva', 'Se', 'a', 'única', 'coisa', 'que', 'de', 'o', 'homem', 'terá', 'certeza', 'é', 'a', 'morte', ';', 'a', 'única', 'certeza', 'do', 'brasileiro', 'é', 'o', 'carnaval', 'no', 'próximo', 'ano', '.']\n"
     ]
    }
   ],
   "source": [
    "### Tokenizacao em portugues\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "text = \"guarda-chuva Se a única coisa que de o homem terá certeza é a morte; a única certeza do brasileiro é o carnaval no próximo ano.\" # Graciliano Ramos\n",
    "\n",
    "result = tokenize.word_tokenize(text, language='portuguese') \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0d2b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           program             \n",
      "programs            program             \n",
      "programmed          program             \n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### 02 - Stemming\n",
    "################################################\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5335ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "programmers         programm            \n",
      "because             becaus              \n",
      "people              peopl               \n"
     ]
    }
   ],
   "source": [
    "example_words = [\"programmers\", \"because\", \"people\"]\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20880b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Masmok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "church\n",
      "aardwolf\n",
      "abacus\n",
      "hardrock\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### 03 - Lemmanization\n",
    "################################################\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "words = ['dogs', 'churches', 'aardwolves', 'abaci', 'hardrock']\n",
    "\n",
    "for w in words:\n",
    "    print(wnl.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0a76ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "### 04 - Stopwords\n",
    "################################################\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(stopwords[:15])\n",
    "len(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e003ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      good      like    movie       not\n",
      "0  0.00000  0.000000  1.00000  0.000000\n",
      "1  0.57735  0.000000  0.57735  0.577350\n",
      "2  0.00000  0.707107  0.00000  0.707107\n",
      "3  0.00000  1.000000  0.00000  0.000000\n",
      "4  1.00000  0.000000  0.00000  0.000000\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### 04 - Caracterizacao das palavras - Tfidf\n",
    "################################################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [\"bad movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good me\"]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1 ,2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "df = pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d3532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### Extra - Lembrando como fazer hashing\n",
    "################################################\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def hash_token(token, b):\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(token.encode()) # UTF-8 encode\n",
    "    return int(hash_object.hexdigest(), 16) % (2**b)\n",
    "\n",
    "# Example usage\n",
    "b = 10  # Number of buckets for hashing\n",
    "token = \"dfadfasdfasdfasdfadfadfadfasdfasdfasdfa\"\n",
    "hashed_value = hash_token(token, b)\n",
    "print(hashed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2c776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor representando 'gato': [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
      "Similaridade das palavras em relacao a 'gato 'gato': [('uiva', 0.09291724115610123), ('persegue', 0.00484249135479331), ('muito', -0.0027540253940969706), ('late', -0.013679751195013523), ('cachorro', -0.028491031378507614), ('rato', -0.05774581804871559), ('lobo', -0.11555545777082443)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### 05 - Caracterizacao das palavras - wordvector\n",
    "################################################\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sentencas\n",
    "sentences = [[\"gato\", \"persegue\", \"rato\"], [\"cachorro\", \"late\", \"muito\"], [\"lobo\", \"uiva\"]]\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences, min_count=1) # ignora palavras com frequencia abaixo de...\n",
    "\n",
    "vector = model.wv['gato']\n",
    "print(\"Vetor representando 'gato':\", vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('gato')\n",
    "print(\"Similaridade das palavras em relacao a 'gato 'gato':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444fa978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'apples': [ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n",
      " -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n",
      " -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n",
      " -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n",
      "  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n",
      "  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n",
      "  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n",
      "  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n",
      " -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n",
      " -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n",
      "  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n",
      "  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n",
      " -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n",
      " -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n",
      " -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n",
      " -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n",
      " -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n",
      " -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n",
      " -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n",
      "  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n",
      "  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n",
      " -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n",
      " -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n",
      " -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n",
      "  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\n",
      "Similar words to 'apples': [('oranges', 0.1459505707025528), ('eating', 0.041577354073524475), ('enjoy', 0.03476494178175926), ('bananas', 0.01915225386619568), ('i', 0.01613469421863556), ('to', 0.008826184086501598), ('eat', 0.004842504393309355), ('like', -0.11410722881555557)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I like to eat apples\",\n",
    "    \"I like bananas\",\n",
    "    \"I enjoy eating oranges\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get the word vector for a word\n",
    "word_vector = model.wv[\"apples\"]\n",
    "print(\"Vector for 'apples':\", word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"apples\")\n",
    "print(\"Similar words to 'apples':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964d1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "##################################################################################################################\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5da5972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n",
      "\n",
      " All the possible Bigrams are \n",
      "[('This', 'is'), ('is', 'a'), ('a', 'dog'), ('This', 'is'), ('is', 'a'), ('a', 'cat'), ('I', 'love'), ('love', 'my'), ('my', 'cat'), ('This', 'is'), ('is', 'my'), ('my', 'name')]\n",
      "\n",
      " Bigrams along with their frequency \n",
      "{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n",
      "\n",
      " Unigrams along with their frequency \n",
      "{'This': 3, 'is': 3, 'a': 2, 'dog': 1, 'cat': 2, 'I': 1, 'love': 1, 'my': 2}\n",
      "\n",
      " Bigrams along with their probability \n",
      "{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'dog'): 0.5, ('a', 'cat'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'cat'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'name'): 0.5}\n",
      "\n",
      " The bigrams in given sentence are \n",
      "[('This', 'is'), ('is', 'my'), ('my', 'cat')]\n",
      "\n",
      "Probablility of sentence \"This is my cat\" = 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "def readData():\n",
    "    data = ['This is a  dog','This is a cat','I love my cat','This is my name ']\n",
    "    dat=[]\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "   listOfBigrams = []\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   for i in range(len(data)-1):\n",
    "      if i < len(data) - 1 and data[i+1].islower():\n",
    "\n",
    "         listOfBigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "         if (data[i], data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i], data[i + 1])] += 1\n",
    "         else:\n",
    "            bigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "      if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = readData()\n",
    "    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "    print(\"\\n All the possible Bigrams are \")\n",
    "    print(listOfBigrams)\n",
    "\n",
    "    print(\"\\n Bigrams along with their frequency \")\n",
    "    print(bigramCounts)\n",
    "\n",
    "    print(\"\\n Unigrams along with their frequency \")\n",
    "    print(unigramCounts)\n",
    "\n",
    "    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "    print(\"\\n Bigrams along with their probability \")\n",
    "    print(bigramProb)\n",
    "    inputList=\"This is my cat\"\n",
    "    splt=inputList.split()\n",
    "    outputProb1 = 1\n",
    "    bilist=[]\n",
    "    bigrm=[]\n",
    "\n",
    "    for i in range(len(splt) - 1):\n",
    "        if i < len(splt) - 1:\n",
    "\n",
    "            bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "    print(\"\\n The bigrams in given sentence are \")\n",
    "    print(bilist)\n",
    "    for i in range(len(bilist)):\n",
    "        if bilist[i] in bigramProb:\n",
    "\n",
    "            outputProb1 *= bigramProb[bilist[i]]\n",
    "        else:\n",
    "\n",
    "            outputProb1 *= 0\n",
    "    print('\\n' + 'Probablility of sentence \\\"This is my cat\\\" = ' + str(outputProb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d23c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 'ART'),\n",
       " ('rato', 'N'),\n",
       " ('roeu', 'V'),\n",
       " ('a', 'ART'),\n",
       " ('roupa', 'N'),\n",
       " ('do', 'KS'),\n",
       " ('rei', 'N'),\n",
       " ('de', 'PREP'),\n",
       " ('Roma', 'NPROP')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "\n",
    "teste_tagger = joblib.load('..\\Datasets\\POS_tagger_brill.pkl')\n",
    "phrase = 'O rato roeu a roupa do rei de Roma'\n",
    "teste_tagger.tag(word_tokenize(phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a15d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     C:\\Users\\Masmok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\mac_morpho.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "nltk.download('mac_morpho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4ff9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(nltk.corpus.mac_morpho.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94a1f261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tinha', 'VAUX'),\n",
       " ('sido', 'PCP'),\n",
       " ('apresentado', 'PCP'),\n",
       " ('por', 'PREP|+'),\n",
       " ('a', 'ART'),\n",
       " ('criadora', 'N'),\n",
       " ('Simone', 'NPROP'),\n",
       " ('Nowak', 'NPROP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "199f76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = len(dataset)\n",
    "tot_train_samples = int(np.ceil(tot*.8))\n",
    "\n",
    "train_data = dataset[:tot_train_samples]\n",
    "test_data = dataset[tot_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "102fffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:8: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_def = t_def.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:9: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af2 = t_affix2.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:10: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af3 = t_affix3.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:11: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af4 = t_affix4.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:12: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af5 = t_affix5.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:13: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af6 = t_affix6.evaluate(test_data) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance dos taggers:\n",
      "         - Default:                     19.68%\n",
      "         - Sufixo tamanho 2 + Default:  27.29%\n",
      "         - Sufixo tamanho 3 + Sufixo 2: 32.23%\n",
      "         - Sufixo tamanho 4 + Sufixo 3: 34.66%\n",
      "         - Sufixo tamanho 5 + Sufixo 4: 36.24%\n",
      "         - Sufixo tamanho 6 + Sufixo 5: 36.71%\n"
     ]
    }
   ],
   "source": [
    "t_def = nltk.DefaultTagger('N')\n",
    "t_affix2 = nltk.AffixTagger(train_data, affix_length=-2, backoff=t_def)\n",
    "t_affix3 = nltk.AffixTagger(train_data, affix_length=-3, backoff=t_affix2)\n",
    "t_affix4 = nltk.AffixTagger(train_data, affix_length=-4, backoff=t_affix3)\n",
    "t_affix5 = nltk.AffixTagger(train_data, affix_length=-5, backoff=t_affix4)\n",
    "t_affix6 = nltk.AffixTagger(train_data, affix_length=-6, backoff=t_affix5)\n",
    "\n",
    "acc_def = t_def.evaluate(test_data) * 100\n",
    "acc_af2 = t_affix2.evaluate(test_data) * 100\n",
    "acc_af3 = t_affix3.evaluate(test_data) * 100\n",
    "acc_af4 = t_affix4.evaluate(test_data) * 100\n",
    "acc_af5 = t_affix5.evaluate(test_data) * 100\n",
    "acc_af6 = t_affix6.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0954d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:8: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_def = t_def.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:9: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af2 = t_affix2.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:10: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af3 = t_affix3.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:11: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af4 = t_affix4.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:12: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af5 = t_affix5.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\429636039.py:13: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_af6 = t_affix6.evaluate(test_data) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance dos taggers:\n",
      "         - Default:                     19.68%\n",
      "         - Sufixo tamanho 2 + Default:  27.29%\n",
      "         - Sufixo tamanho 3 + Sufixo 2: 32.23%\n",
      "         - Sufixo tamanho 4 + Sufixo 3: 34.66%\n",
      "         - Sufixo tamanho 5 + Sufixo 4: 36.24%\n",
      "         - Sufixo tamanho 6 + Sufixo 5: 36.71%\n"
     ]
    }
   ],
   "source": [
    "t_def = nltk.DefaultTagger('N')\n",
    "t_affix2 = nltk.AffixTagger(train_data, affix_length=-2, backoff=t_def)\n",
    "t_affix3 = nltk.AffixTagger(train_data, affix_length=-3, backoff=t_affix2)\n",
    "t_affix4 = nltk.AffixTagger(train_data, affix_length=-4, backoff=t_affix3)\n",
    "t_affix5 = nltk.AffixTagger(train_data, affix_length=-5, backoff=t_affix4)\n",
    "t_affix6 = nltk.AffixTagger(train_data, affix_length=-6, backoff=t_affix5)\n",
    "\n",
    "acc_def = t_def.evaluate(test_data) * 100\n",
    "acc_af2 = t_affix2.evaluate(test_data) * 100\n",
    "acc_af3 = t_affix3.evaluate(test_data) * 100\n",
    "acc_af4 = t_affix4.evaluate(test_data) * 100\n",
    "acc_af5 = t_affix5.evaluate(test_data) * 100\n",
    "acc_af6 = t_affix6.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19fbad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\1487974386.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_uni = t_uni.evaluate(test_data) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance dos taggers:\n",
      "         - Default:                     19.68%\n",
      "         - Sufixo tamanho 2 + Default:  27.29%\n",
      "         - Sufixo tamanho 3 + Sufixo 2: 32.23%\n",
      "         - Sufixo tamanho 4 + Sufixo 3: 34.66%\n",
      "         - Sufixo tamanho 5 + Sufixo 4: 36.24%\n",
      "         - Sufixo tamanho 6 + Sufixo 5: 36.71%\n",
      "         - Unigrama + Sufixo 6:         83.70%\n"
     ]
    }
   ],
   "source": [
    "t_uni = nltk.UnigramTagger(train_data, backoff=t_affix5)\n",
    "\n",
    "acc_uni = t_uni.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%\n",
    "         - Unigrama + Sufixo 6:         {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6,\n",
    "                                                          acc_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab303543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\2443458364.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_bi = t_bi.evaluate(test_data) * 100\n",
      "C:\\Users\\Masmok\\AppData\\Local\\Temp\\ipykernel_13536\\2443458364.py:5: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  acc_tri = t_tri.evaluate(test_data) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance dos taggers:\n",
      "         - Default:                     19.68%\n",
      "         - Sufixo tamanho 2 + Default:  27.29%\n",
      "         - Sufixo tamanho 3 + Sufixo 2: 32.23%\n",
      "         - Sufixo tamanho 4 + Sufixo 3: 34.66%\n",
      "         - Sufixo tamanho 5 + Sufixo 4: 36.24%\n",
      "         - Sufixo tamanho 6 + Sufixo 5: 36.71%\n",
      "         - Unigrama + Sufixo 6:         83.70%\n",
      "         - Bigrama + Unigrama:          85.18%\n",
      "         - Trigrama + Bigrama:          85.19%\n"
     ]
    }
   ],
   "source": [
    "t_bi = nltk.BigramTagger(train_data, backoff=t_uni)\n",
    "t_tri = nltk.TrigramTagger(train_data, backoff=t_bi)\n",
    "\n",
    "acc_bi = t_bi.evaluate(test_data) * 100\n",
    "acc_tri = t_tri.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%\n",
    "         - Unigrama + Sufixo 6:         {:.2f}%\n",
    "         - Bigrama + Unigrama:          {:.2f}%\n",
    "         - Trigrama + Bigrama:          {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6,\n",
    "                                                          acc_uni, acc_bi, acc_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d7705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(t_bi, open('..\\Datasets\\POS_tagger_brill.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e48dc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b288f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ('Rainy', 'Sunny')\n",
    "\n",
    "observations = ('walk', 'shop', 'clean')\n",
    "\n",
    "start_probability = {'Rainy': 0.6, 'Sunny': 0.4}\n",
    "\n",
    "transition_probability = {\n",
    "'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},\n",
    "'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},\n",
    "}\n",
    "\n",
    "emission_probability = {\n",
    "'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},\n",
    "'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0612c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Rainy': {'prob': 0.06, 'prev': None},\n",
       "  'Sunny': {'prob': 0.24, 'prev': None}},\n",
       " {'Rainy': {'prob': 0.038400000000000004, 'prev': 'Sunny'},\n",
       "  'Sunny': {'prob': 0.043199999999999995, 'prev': 'Sunny'}},\n",
       " {'Rainy': {'prob': 0.01344, 'prev': 'Rainy'},\n",
       "  'Sunny': {'prob': 0.0025919999999999997, 'prev': 'Sunny'}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(observations,\n",
    "         states,\n",
    "         start_probability,\n",
    "         transition_probability,\n",
    "         emission_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e0e15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "554f523d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/dtspeech/DTSpeech.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/dtspeech/DTSpeech.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m corpus \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(corpus)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/dtspeech/DTSpeech.txt'"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "data = open('../input/dtspeech/DTSpeech.txt').read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25a6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
