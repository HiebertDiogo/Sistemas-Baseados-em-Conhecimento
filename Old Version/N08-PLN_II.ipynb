{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0adcb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "##### Notebook Processamento de Linguagem natural (PLN)\n",
    "##### Baseado em:\n",
    "## Natural Language Processing with Python (book)\n",
    "##\n",
    "##############################################################################################################\n",
    "## Objetivos:\n",
    "##   Mostrar varios metodos de linguagem natural utilizando Python\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4e1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 01 - Modelos de Linguagem - Bigramas\n",
    "################################################\n",
    "\n",
    "def readData():\n",
    "    data = ['Este eh um cachorro','Este eh um gato','Eu amo meu gato','Este eh meu nome']\n",
    "    dat=[]\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    #print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "   listOfBigrams = []\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   for i in range(len(data)-1):\n",
    "      if i < len(data) - 1 and data[i+1].islower():\n",
    "\n",
    "         listOfBigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "         if (data[i], data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i], data[i + 1])] += 1\n",
    "         else:\n",
    "            bigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "      if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75100d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Todos os bigramas possiveis sao: \n",
      "[('Este', 'eh'), ('eh', 'um'), ('um', 'cachorro'), ('Este', 'eh'), ('eh', 'um'), ('um', 'gato'), ('Eu', 'amo'), ('amo', 'meu'), ('meu', 'gato'), ('Este', 'eh'), ('eh', 'meu'), ('meu', 'nome')]\n",
      "\n",
      " Bigramas e suas frequencias: \n",
      "{('Este', 'eh'): 3, ('eh', 'um'): 2, ('um', 'cachorro'): 1, ('um', 'gato'): 1, ('Eu', 'amo'): 1, ('amo', 'meu'): 1, ('meu', 'gato'): 1, ('eh', 'meu'): 1, ('meu', 'nome'): 1}\n",
      "\n",
      " Unigramas e suas frequencias: \n",
      "{'Este': 3, 'eh': 3, 'um': 2, 'cachorro': 1, 'gato': 2, 'Eu': 1, 'amo': 1, 'meu': 2}\n",
      "\n",
      " Bigramas e suas probabilidades: \n",
      "{('Este', 'eh'): 1.0, ('eh', 'um'): 0.6666666666666666, ('um', 'cachorro'): 0.5, ('um', 'gato'): 0.5, ('Eu', 'amo'): 1.0, ('amo', 'meu'): 1.0, ('meu', 'gato'): 0.5, ('eh', 'meu'): 0.3333333333333333, ('meu', 'nome'): 0.5}\n",
      "\n",
      " Os bigramas na sentenca de entrada sao: \n",
      "[('Este', 'eh'), ('eh', 'meu'), ('meu', 'gato')]\n",
      "\n",
      "Probabilidade da sentenca Este eh meu gato= 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "data = readData()\n",
    "listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "print(\"\\n Todos os bigramas possiveis sao: \")\n",
    "print(listOfBigrams)\n",
    "\n",
    "print(\"\\n Bigramas e suas frequencias: \")\n",
    "print(bigramCounts)\n",
    "\n",
    "print(\"\\n Unigramas e suas frequencias: \")\n",
    "print(unigramCounts)\n",
    "\n",
    "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "print(\"\\n Bigramas e suas probabilidades: \")\n",
    "print(bigramProb)\n",
    "inputList=\"Este eh meu gato\"\n",
    "splt=inputList.split()\n",
    "outputProb1 = 1\n",
    "bilist=[]\n",
    "bigrm=[]\n",
    "\n",
    "for i in range(len(splt) - 1):\n",
    "    if i < len(splt) - 1:\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "    \n",
    "print(\"\\n Os bigramas na sentenca de entrada sao: \")\n",
    "print(bilist)\n",
    "for i in range(len(bilist)):\n",
    "    if bilist[i] in bigramProb:\n",
    "        outputProb1 *= bigramProb[bilist[i]]\n",
    "    else:\n",
    "        outputProb1 *= 0\n",
    "print('\\n' + 'Probabilidade da sentenca '+inputList+ '= ' + str(outputProb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3c8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 02 - Smoothing\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3cc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from numpy.random import choice \n",
    "from tqdm import tqdm\n",
    "\n",
    "class Bigram():\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "        self.context = defaultdict(Counter)\n",
    "        self.start_count = 0\n",
    "        self.token_count = 0\n",
    "        self.vocab_count = 0\n",
    "    \n",
    "    def convert_sentence(self, sentence):\n",
    "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "    \n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
    "                self.unigram_counts[word] += 1\n",
    "            self.start_count += 1\n",
    "            \n",
    "        # collect bigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "            for bigram in bigram_list:\n",
    "                self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "                self.context[bigram[1]][bigram[0]] += 1\n",
    "        self.token_count = sum(self.unigram_counts.values())\n",
    "        self.vocab_count = len(self.unigram_counts.keys())\n",
    "        \n",
    "    def generate_sentence(self):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        while current_word != \"</s>\":\n",
    "            prev_word = current_word\n",
    "            prev_word_counts = self.bigram_counts[prev_word]\n",
    "            # obtain bigram probability distribution given the previous word\n",
    "            bigram_probs = []\n",
    "            total_counts = float(sum(prev_word_counts.values()))\n",
    "            for word in prev_word_counts:\n",
    "                bigram_probs.append(prev_word_counts[word] / total_counts)\n",
    "            # sample the next word\n",
    "            current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
    "            sentence.append(current_word)\n",
    "            \n",
    "        sentence = \" \".join(sentence[1:-1])\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761686ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Masmok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "eighteenth-century england , fair .\n",
      "Sentence 2\n",
      "and feel that proposition , chairman of mops , cones of the audience , who decides to say the comportment of the maverick when oncoming traffic coming after warning system to improve pax-ordo of the ground , no one big money over a gigantic voice .\n",
      "Sentence 3\n",
      "however , and arthur williams's might logically , separately , you own order , either a surface absurdity of unobtrusive use of his present pool is authorized for hanover ) , whining voice so that -- idleness and prompt time for a surface craft is flesh , bud freeman `` a day before the emergence of the long experience at hand every movement until mid-june .\n",
      "Sentence 4\n",
      "afterwards , clutching the hypocritical self-aggrandizement .\n",
      "Sentence 5\n",
      "it was fifteen dollars under that a difference in the body of legends that the long-range ballistic missile .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown') # Corpus com um milhao de palavras de textos americanos lancados em 1961\n",
    "\n",
    "bigram = Bigram()\n",
    "bigram.get_counts(brown.sents())\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(bigram.generate_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57832c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "def split_train_test():\n",
    "    sents = list(brown.sents())\n",
    "    shuffle(sents)\n",
    "    cutoff = int(0.8*len(sents))\n",
    "    training_set = sents[:cutoff]\n",
    "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "    return training_set, test_set\n",
    "\n",
    "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in tqdm(sentences):\n",
    "        test_token_count += len(sentence) + 1 # para considerar o token <fim-de-arquivo>\n",
    "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
    "    return math.exp(-total_log_prob / test_token_count)\n",
    "\n",
    "training_set, test_set = split_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719bae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicao da funcao de smoothing\n",
    "\n",
    "def laplacian_smoothing(sentence, bigram, parameter):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4184fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11468/11468 [00:00<00:00, 15363.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3490.949863099908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execucao principal\n",
    "\n",
    "bigram_laplacian_smoothing = Bigram()\n",
    "bigram_laplacian_smoothing.get_counts(training_set) # cria o modelo\n",
    "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
    "print(plex_laplacian_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13900e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 'ART'),\n",
       " ('rato', 'N'),\n",
       " ('roeu', 'V'),\n",
       " ('a', 'ART'),\n",
       " ('roupa', 'N'),\n",
       " ('do', 'KS'),\n",
       " ('rei', 'N'),\n",
       " ('de', 'PREP'),\n",
       " ('Roma', 'NPROP')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "### 03 - PoS tagging\n",
    "################################################\n",
    "\n",
    "# baixar o modelo: POS_tagger_brill.pkl\n",
    "# POS_tagger_bigram.pkl (tentar outros)\n",
    "# https://github.com/inoueMashuu/POS-tagger-portuguese-nltk/tree/master/trained_POS_taggers\n",
    "\n",
    "\n",
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "\n",
    "teste_tagger = joblib.load('..\\Datasets\\POS_tagger_brill.pkl')\n",
    "phrase = 'O rato roeu a roupa do rei de Roma'\n",
    "teste_tagger.tag(word_tokenize(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79825c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 04 - Viterbi algorithm\n",
    "################################################\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0] [st] = {\"prob\": start_p[st] * emit_p[st] [obs[0]], \"prev\": None}\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1] [states[0]] [\"prob\"] * trans_p[states[0]] [st] * emit_p[st] [obs[t]]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1] [prev_st] [\"prob\"] * trans_p[prev_st] [st] * emit_p[st] [obs[t]]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "\n",
    "            max_prob = max_tr_prob\n",
    "            V[t] [st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "\n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1] [previous] [\"prev\"])\n",
    "        previous = V[t + 1] [previous] [\"prev\"]\n",
    "\n",
    "    print (\"A sequencia de estados foi: \" + \" \".join(opt) + \" tendo probabilidade de %s\" % max_prob + \" (maior probabilidade)\")\n",
    "\n",
    "def dptable(V):\n",
    "    # Print a table of steps from dictionary\n",
    "    yield \" \" * 5 + \"     \".join((\"%3d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%lf\" % v[state] [\"prob\"]) for v in V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0d27420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0       1       2\n",
      "NOUN: 0.24000 0.00450 0.01058\n",
      "VERB: 0.03000 0.11760 0.00940\n",
      "ADJ: 0.09000 0.00960 0.01764\n",
      "A sequencia de estados foi: NOUN VERB ADJ tendo probabilidade de 0.017639999999999996 (maior probabilidade)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the states (POS tags) and observation symbols (words)\n",
    "states = ['NOUN', 'VERB', 'ADJ']\n",
    "observations = ['dog', 'runs', 'fast']\n",
    "\n",
    "# Define the transition probabilities\n",
    "transition_probabilities = {\n",
    "    'NOUN': {'NOUN': 0.1, 'VERB': 0.7, 'ADJ': 0.2},\n",
    "    'VERB': {'NOUN': 0.3, 'VERB': 0.4, 'ADJ': 0.3},\n",
    "    'ADJ': {'NOUN': 0.5, 'VERB': 0.1, 'ADJ': 0.4}\n",
    "}\n",
    "\n",
    "# Define the emission probabilities\n",
    "emission_probabilities = {\n",
    "    'NOUN': {'dog': 0.6, 'runs': 0.1, 'fast': 0.3},\n",
    "    'VERB': {'dog': 0.1, 'runs': 0.7, 'fast': 0.2},\n",
    "    'ADJ': {'dog': 0.3, 'runs': 0.2, 'fast': 0.5}\n",
    "}\n",
    "\n",
    "# Define the initial probabilities\n",
    "start_probabilities = {'NOUN': 0.4, 'VERB': 0.3, 'ADJ': 0.3}\n",
    "\n",
    "viterbi(observations,\n",
    "           states,\n",
    "           start_probabilities,\n",
    "           transition_probabilities,\n",
    "           emission_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96d1b123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0       1       2\n",
      "Chovend: 0.06000 0.03840 0.01344\n",
      "Ensolar: 0.24000 0.04320 0.00259\n",
      "A sequencia de estados foi: Ensolarado Chovendo Chovendo tendo probabilidade de 0.01344 (maior probabilidade)\n"
     ]
    }
   ],
   "source": [
    "states = ('Chovendo', 'Ensolarado')\n",
    "\n",
    "observations = ('Andar', 'Comprar', 'LimparCasa')\n",
    "\n",
    "start_probability = {'Chovendo': 0.6, 'Ensolarado': 0.4}\n",
    "\n",
    "transition_probability = {\n",
    "'Chovendo' : {'Chovendo': 0.7, 'Ensolarado': 0.3},\n",
    "'Ensolarado' : {'Chovendo': 0.4, 'Ensolarado': 0.6},\n",
    "}\n",
    "\n",
    "emission_probability = {\n",
    "'Chovendo' : {'Andar': 0.1, 'Comprar': 0.4, 'LimparCasa': 0.5},\n",
    "'Ensolarado' : {'Andar': 0.6, 'Comprar': 0.3, 'LimparCasa': 0.1},\n",
    "}\n",
    "\n",
    "viterbi(observations,\n",
    "           states,\n",
    "           start_probability,\n",
    "           transition_probability,\n",
    "           emission_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d30e1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 05 - Engenharia de caracteristicas\n",
    "################################################\n",
    "\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_capital_letters(text):\n",
    "    return sum(1 for char in text if char.isupper())\n",
    "\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "\n",
    "def count_punctuations(text):\n",
    "# retorna um dicinario com 32 pontuacoes e contadores associados\n",
    "    punctuations=\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d\n",
    "\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall('\"([^\"]*)\"', text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count\n",
    "\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x) \n",
    "\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('portuguese'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7cece36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Ó mar salgado, quanto do teu sal, são lágrimas de Portugal! Por te cruzarmos, quantas mães choraram, quantos filhos em vão rezaram! Quantas noivas ficaram por casar, para que fosses nosso, ó mar! Valeu a pena? Tudo vale a pena, se a alma não é pequena. Quem quer passar além do Bojador, tem que passar além da dor. Deus ao mar o perigo e o abismo deu. Mas nele é que espelhou o céu.\"\n",
    "\n",
    "dict_features = {}\n",
    "\n",
    "dict_features['char_count'] = count_chars(text)\n",
    "dict_features['word_count'] = count_words(text)\n",
    "dict_features['stopword_count'] = count_stopwords(text)\n",
    "dict_features['unique_word_count'] = count_unique_words(text)\n",
    "dict_features['sent_count'] = count_sent(text)\n",
    "\n",
    "dict_features['avg_wordlength'] = int(dict_features['char_count'])/int(dict_features['word_count'])\n",
    "dict_features['avg_sentlength'] = dict_features['word_count']/dict_features['sent_count']\n",
    "dict_features['unique_vs_words'] = dict_features['unique_word_count']/dict_features['word_count']\n",
    "dict_features['stopwords_vs_words'] = dict_features['stopword_count']/dict_features['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "653fc3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_count': 382,\n",
       " 'word_count': 74,\n",
       " 'stopword_count': 27,\n",
       " 'unique_word_count': 63,\n",
       " 'sent_count': 8,\n",
       " 'avg_wordlength': 5.162162162162162,\n",
       " 'avg_sentlength': 9.25,\n",
       " 'unique_vs_words': 0.8513513513513513,\n",
       " 'stopwords_vs_words': 0.36486486486486486}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a7146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
